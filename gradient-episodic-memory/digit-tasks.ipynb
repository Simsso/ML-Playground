{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b43e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import accumulate\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch import Tensor\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 250\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b42c44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69b0a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 245
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0].reshape(8, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94e8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577766a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model() -> torch.nn.Module:\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(64, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 10),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea1e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequence of tasks from the digits dataset.\n",
    "tasks: list[tuple[torch.Tensor, torch.Tensor]] = []\n",
    "\n",
    "# Task 1 are digits 0 and 1.\n",
    "tasks.append((x[y <= 1], y[y <= 1]))\n",
    "\n",
    "# Task 2 are digits 2 to 5.\n",
    "tasks.append((x[(y >= 2) & (y <= 5)], y[(y >= 2) & (y <= 5)]))\n",
    "\n",
    "# Task 3 are the remaining digits except 9.\n",
    "tasks.append((x[(y >= 6) & (y < 9)], y[(y >= 6) & (y < 9)]))\n",
    "\n",
    "# Task 4 is digit 9.\n",
    "tasks.append((x[y == 9], y[y == 9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "526d0e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([360, 64]), torch.Size([723, 64]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks[0][0].shape, tasks[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694d21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_clone_model(old_model: torch.nn.Module) -> torch.nn.Module:\n",
    "    model = initialize_model()\n",
    "    model.load_state_dict(old_model.state_dict())\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.clone().detach()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7440eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "lr = 0.01\n",
    "\n",
    "\n",
    "def compute_acc(model: torch.nn.Module, x: Tensor, y: Tensor) -> float:\n",
    "    acc = (model(x).argmax(dim=1) == y).float().mean()\n",
    "    return acc.item()\n",
    "\n",
    "\n",
    "def train_on_task(old_model: torch.nn.Module, x: Tensor, y: Tensor) -> torch.nn.Module:\n",
    "    # Each task is trained on a new (copied) set of weights.\n",
    "    model = deep_clone_model(old_model)\n",
    "\n",
    "    # Recreate the optimizer for each task.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = cross_entropy(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f77c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_model = initialize_model()\n",
    "\n",
    "task_1_model = train_on_task(rand_model, *tasks[0])\n",
    "task_2_model = train_on_task(task_1_model, *tasks[1])\n",
    "\n",
    "# Compute accuracy matrix\n",
    "models = [rand_model, task_1_model, task_2_model]\n",
    "acc_mat = torch.zeros((len(models), len(tasks)))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    for j, task in enumerate(tasks):\n",
    "        acc_mat[i, j] = compute_acc(model, *task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba9b391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 366
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the acc mat and show the values in the cells.\n",
    "plt.imshow(acc_mat);\n",
    "for i in range(len(models)):\n",
    "    for j in range(len(tasks)):\n",
    "        plt.text(j, i, f\"{acc_mat[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "plt.xticks(range(len(tasks)), [f\"Task {i}\" for i in range(1, len(tasks) + 1)])\n",
    "plt.yticks(range(len(models)), [\"Rand Init\"] + [f\"Post Task {i}\" for i in range(1, len(models))]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eae90688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1: Train a GEM over an ordered continuum of data.\n",
    "\n",
    "\n",
    "def get_task_grad_vec(model: torch.nn.Module, x: Tensor, y: Tensor) -> Tensor:\n",
    "    mean_loss = cross_entropy(model(x), y).mean()\n",
    "    grads = torch.autograd.grad(mean_loss, model.parameters())\n",
    "    return torch.concat([g.flatten() for g in grads])\n",
    "\n",
    "\n",
    "def get_lagrange_multiplier(G: Tensor, g: Tensor) -> Tensor:\n",
    "    # Solves Eq. (11).\n",
    "    # GG^T is TxT (T = |tasks|)\n",
    "    # min_v (1/2v^T GG^T v + g^T G^T v), s.t. v >= 0\n",
    "    num_epochs = 1000\n",
    "    lr = 0.01\n",
    "\n",
    "    GG = G @ G.T\n",
    "    Gg = G @ g\n",
    "\n",
    "    v = torch.rand(G.shape[0], requires_grad=True)\n",
    "\n",
    "    # v = torch.ones(G.shape[0], requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([v], lr=lr)\n",
    "\n",
    "    eps = 10e-2\n",
    "    v = v * eps\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        v.grad = GG @ v + Gg @ v\n",
    "        optimizer.step()\n",
    "        v = v.clip(min=0)  # Project onto the non-negative orthant.\n",
    "\n",
    "    return v\n",
    "\n",
    "\n",
    "def project_grad(G: Tensor, g: Tensor) -> list[Tensor]:\n",
    "    v_star = get_lagrange_multiplier(G, g)\n",
    "    g_tilde = G.T @ v_star + g\n",
    "    return g_tilde\n",
    "\n",
    "\n",
    "def gem_grad_update_(\n",
    "    model: torch.nn.Module, task_memory: list[tuple[Tensor, Tensor]]\n",
    ") -> None:\n",
    "    \"\"\"Updates the gradients of model in-place using GEM.\"\"\"\n",
    "    if not task_memory:\n",
    "        return\n",
    "\n",
    "    # Assumes model.parameters() returns params in a stable order.\n",
    "    g = torch.concat([p.grad.flatten() for p in model.parameters()])\n",
    "    G = torch.stack([get_task_grad_vec(model, x, y) for x, y in task_memory])\n",
    "\n",
    "    if torch.all(G @ g > 0):\n",
    "        return\n",
    "\n",
    "    g_tilde = project_grad(G, g)\n",
    "\n",
    "    print(f\"{G @ g_tilde=}\")\n",
    "\n",
    "    # Slice the single column large proejcted grad into the parts for each param.\n",
    "    g_sizes = [p.numel() for p in model.parameters()]\n",
    "    g_offsets = [0] + list(accumulate(g_sizes))\n",
    "\n",
    "    # In-place update of the gradients.\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        g_tilde_for_p = g_tilde[g_offsets[i] : g_offsets[i + 1]]\n",
    "        print(f\"{torch.norm(p.grad.flatten() - g_tilde_for_p)=}\")\n",
    "        p.grad = g_tilde_for_p.reshape(p.grad.shape)\n",
    "\n",
    "\n",
    "def sample_from_task(x: Tensor, y: Tensor, ratio: float = 1.0) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Samples ratio examples from the task.\"\"\"\n",
    "    num_samples = int(len(x) * ratio)\n",
    "    idx = torch.randperm(len(x))[:num_samples]\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "\n",
    "def train_with_gem(\n",
    "    initial_model: torch.nn.Module, tasks: list[tuple[Tensor, Tensor]]\n",
    ") -> list[torch.nn.Module]:\n",
    "    num_epochs = 50\n",
    "    lr = 0.01\n",
    "\n",
    "    # M_t <- {sample of examples} for all tasks\n",
    "    task_memory = []\n",
    "\n",
    "    models = [initial_model]\n",
    "    for task_idx, (x, y) in enumerate(tasks):\n",
    "        print(f\"Learning task {task_idx}\")\n",
    "        if task_idx > 0:\n",
    "            task_memory.append(sample_from_task(*tasks[task_idx - 1]))\n",
    "\n",
    "        # Each task is trained on a new (copied) set of weights.\n",
    "        model = deep_clone_model(models[-1])\n",
    "        # Recreate the optimizer for each task.\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        for _ in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            loss = torch.nn.functional.cross_entropy(model(x), y)\n",
    "            loss.backward()\n",
    "            gem_grad_update_(model, task_memory)\n",
    "            optimizer.step()\n",
    "\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db6ab5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning task 0\n",
      "Learning task 1\n",
      "G @ g_tilde=tensor([-3.7478e-05], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.6792e-09, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0., grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(7.3857e-10, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(2.9104e-11, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.0668e-16, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0., grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-11.9829], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0125, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0002, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0108, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(9.4914e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0305, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0002, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-2.1866], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2543, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0041, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3073, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0109, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4527, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0356, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-8.4880], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0402, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0007, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0391, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0016, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0427, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0057, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-5.8825], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4275, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0071, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3950, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0169, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2828, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0546, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-11.6716], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0558, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0009, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0541, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0023, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0349, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0068, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-9.2548], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1471, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0023, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1805, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0077, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1154, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0199, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-7.9623], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0895, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0014, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1256, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0052, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0837, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0127, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-9.2576], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0044, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(7.0044e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0063, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0003, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0044, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0006, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-2.9372], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3355, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0055, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4962, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0193, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3829, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0434, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-7.7298], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1352, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0023, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2118, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0080, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1819, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0180, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-7.6175], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0323, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0006, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0517, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0019, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0454, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0041, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-1.2051], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1488, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0026, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2347, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0084, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2005, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0167, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-1.2448], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0318, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0006, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0455, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0016, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0385, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0030, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([23.1251], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5421, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0100, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6876, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0234, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5978, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0393, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([43.7457], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9889, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0181, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.2987, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0434, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1780, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0723, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([56.9720], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1435, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0206, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.5876, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0528, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.4912, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0880, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([10.6464], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2690, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0048, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4080, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0138, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3833, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0224, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([4.6598], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2178, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0039, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3628, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0125, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3344, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0194, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([7.1846], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3316, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0061, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5489, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0193, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4941, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0284, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([1.7820], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3849, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0072, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5791, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0207, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5216, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0294, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-1.1016], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5313, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0100, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7356, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0267, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6899, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0378, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-10.4015], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3890, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0072, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5524, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0203, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5159, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0278, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([0.9038], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6227, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0114, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9737, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0361, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.8723, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0469, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-4.2935], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4129, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0068, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7188, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0268, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6524, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0348, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-15.5845], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0097, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0001, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0168, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0006, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0159, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0008, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-5.5952], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0674, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0009, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1191, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0045, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1158, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0061, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([17.4645], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3742, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0052, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6854, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0261, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6689, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0357, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([7.5803], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1995, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0030, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3362, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0130, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3242, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0175, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([18.3409], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4091, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0068, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5751, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0227, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5442, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0296, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([27.0869], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6203, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0107, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7425, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0298, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6956, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0380, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([20.7600], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5243, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0090, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5880, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0239, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5557, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0302, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([10.6461], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3511, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0058, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3910, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0160, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3783, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0203, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-4.2412], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0203, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0003, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0233, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0010, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0233, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0012, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([3.0513], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3204, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0044, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3674, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0152, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3804, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0198, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-5.3196], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0714, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0009, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0788, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0033, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0846, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0043, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-0.7809], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3068, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0035, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3223, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0134, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3562, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0179, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-4.8256], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1016, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0013, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0994, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0042, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1109, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0055, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-2.2520], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1832, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0026, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1635, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0070, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1805, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0091, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([5.7913], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5182, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0079, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4331, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0188, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4686, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0240, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([2.7269], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3258, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0051, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2677, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0117, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2871, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0149, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([4.0084], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3751, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0057, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3164, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0139, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3416, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0178, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([4.5993], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4126, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0058, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3590, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0159, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3945, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0207, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-0.6861], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1496, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0019, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1292, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0057, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1453, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0076, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Learning task 2\n",
      "G @ g_tilde=tensor([-59.7738,   5.1399], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3037, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0039, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2419, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0107, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2780, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0146, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-15.9561,   7.3814], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5854, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0106, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3337, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0190, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3298, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0246, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-24.7645,  -5.2997], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4685, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0087, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1667, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0116, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1413, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0137, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ -9.6434, -11.2622], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4506, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0075, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2302, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0244, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2176, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0405, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-10.0052,  -5.6866], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2296, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0042, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0581, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0070, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0471, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0070, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 1.0385, -2.2863], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3985, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0067, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1393, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0191, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0719, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0479, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-2.6159, -3.7235], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1494, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0026, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0477, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0048, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0245, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0109, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-4.1291, -5.5073], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0574, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0010, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0196, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0022, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0106, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0052, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-4.0536, -6.8780], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4298, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0076, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1246, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0169, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0957, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0495, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ -7.9387, -10.1672], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2562, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0049, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1002, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0131, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0620, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0273, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ -9.7519, -13.6235], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5173, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0097, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2067, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0237, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1502, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0485, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-7.7180, -8.5784], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.8601, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0166, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4373, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0442, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2293, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0634, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 0.8570, -0.5685], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6690, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0126, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3289, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0281, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2155, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0438, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([21.1801, 21.0286], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.8369, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0160, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5079, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0311, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2704, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0333, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([24.4898, 22.7525], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1724, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0223, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7479, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0427, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4007, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0455, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([20.6931, 39.8064], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.5389, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0295, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1373, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0600, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5692, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0601, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([27.6694, 35.7376], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.5662, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0291, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1778, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0593, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6534, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0665, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 0.5108, -1.8594], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7011, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0127, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5085, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0251, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3098, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0312, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 1.7971, 43.1370], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.6221, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0310, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.3271, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0629, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6975, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0645, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([28.6143, 76.3320], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(2.4547, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0472, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(2.0509, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0963, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.0669, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0953, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([51.5900, 62.8881], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(2.3660, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0455, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(2.0798, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0988, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.0418, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0974, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([33.6612, 35.0274], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.5972, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0308, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.6418, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0783, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7788, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0773, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([29.6501, 21.3763], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.8694, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0354, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.7310, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0839, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9830, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0974, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ -9.9349, -10.9896], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1902, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0224, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.3737, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0662, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.8170, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0816, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-12.6823, -16.3288], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1425, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0212, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.3958, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0670, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9762, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0933, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-14.1593, -15.3826], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7008, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0130, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9674, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0453, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7241, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0639, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-9.3665, -2.3674], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6292, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0115, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9635, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0444, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6972, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0598, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([19.7608,  6.1487], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.0082, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0186, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.0732, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0505, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1216, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0835, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([27.0996, 11.9980], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.2132, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0223, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1757, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0563, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.2848, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0951, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([10.5078, 12.7266], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9607, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0177, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1005, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0546, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9939, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0803, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([12.2124,  8.3260], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.2821, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0233, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.0913, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0567, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1680, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0927, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-13.5198,  -1.0002], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5826, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0105, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5240, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0287, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5065, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0447, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-9.0424, -7.0827], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6001, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0103, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3555, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0193, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4689, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0429, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 3.2679, -0.3448], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7903, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0140, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5870, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0307, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7797, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0585, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([16.6298,  6.3156], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.0077, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0178, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.8392, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0419, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1766, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0803, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([17.8329,  7.8360], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9494, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0166, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.8735, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0424, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.2651, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0825, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 8.7023, -1.4053], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7315, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0121, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5889, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0263, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9545, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0610, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-6.5676, -3.8496], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3183, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0050, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2671, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0120, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4409, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0302, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-7.9757,  0.7639], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2782, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0042, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3150, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0154, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4775, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0333, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-6.1380, 15.1866], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7375, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0139, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7414, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0375, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.8261, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0552, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-6.5092, -0.9622], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1864, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0035, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1800, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0092, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2013, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0136, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 6.7739, 11.0113], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7997, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0147, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7799, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0390, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9825, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0623, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([12.8223,  3.1926], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7924, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0130, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6898, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0317, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9960, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0584, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 3.2760, -4.9114], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4734, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0072, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3267, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0137, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4859, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0309, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Learning task 3\n",
      "G @ g_tilde=tensor([ 40.8976, 192.0619, -35.7602], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4225, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0063, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4304, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0224, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5182, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0381, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-62.3146, -24.8539, -77.5058], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9088, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0159, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4808, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0209, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2501, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0245, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([  6.6971,   3.0473, -38.8032], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.8066, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0140, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4708, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0195, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2190, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0310, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 10.2095,  12.9493, -23.4436], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5508, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0098, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4712, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0209, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1626, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0321, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-11.7929,  11.5564, -17.3793], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4332, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0073, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4802, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0214, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1803, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0426, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-7.7761,  7.2990,  8.6287], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6927, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0124, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5274, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0223, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1751, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0443, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-0.7003, 20.0895, 70.7664], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9398, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0172, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7286, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0267, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1647, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0436, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-3.5218, 18.5262, 78.7706], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7429, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0133, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6692, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0232, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1237, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0324, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-6.2063,  7.6027, 48.0439], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7066, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0123, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.8436, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0291, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1373, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0436, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-8.0744,  0.2935, 19.2080], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3257, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0054, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5447, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0185, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0787, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0360, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-6.3749, -1.0335,  7.0486], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1709, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0019, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3424, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0116, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0482, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0202, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-4.0789, -1.6372,  2.4853], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1195, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0015, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1926, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0067, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0249, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0126, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-1.2940, -0.5511,  0.1756], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1048, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0014, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2090, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0068, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0263, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0153, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 2.6338,  0.6528, -0.1800], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2861, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0038, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6451, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0201, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0608, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0535, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 0.9485,  0.4322, -0.3688], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1615, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0021, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3419, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0109, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0233, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0295, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([1.5456, 0.2933, 0.2506], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2508, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0036, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4369, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0149, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0332, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0281, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 1.6059, -0.4098,  0.4911], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2839, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0047, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3912, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0139, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0338, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0353, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 2.1052, -0.2092,  0.7497], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3704, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0061, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4626, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0172, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0524, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0438, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([1.6207, 0.2680, 0.6947], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3389, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0056, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4590, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0165, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0507, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0341, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([0.2135, 0.3726, 0.0577], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1869, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0030, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2739, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0095, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0292, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0191, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([0.9300, 0.3618, 0.7190], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2977, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0050, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4538, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0151, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0400, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0269, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-0.6902,  0.1274, -0.1240], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1081, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0018, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1724, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0056, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0149, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0086, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 0.5450, -0.3698,  0.4170], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1974, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0034, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2399, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0085, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0367, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0208, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([0.5380, 0.7435, 0.6076], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2708, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0045, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3492, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0116, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0507, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0289, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([0.0450, 0.6023, 0.5204], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2391, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0042, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2948, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0091, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0383, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0244, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 0.9543, -0.3165,  0.9605], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2341, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0040, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3088, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0096, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0490, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0263, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 3.8863, -2.7792,  2.1327], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2910, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0048, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3782, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0129, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0717, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0366, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 3.2404, -3.2228,  2.0686], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2169, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0036, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2665, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0090, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0535, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0363, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 1.9065, -4.8512,  0.8803], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2120, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0032, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2372, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0087, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0878, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0425, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-0.4501, -8.0865, -1.3605], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1708, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0024, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2424, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0079, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0937, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0425, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-3.5037, -6.2409, -2.6821], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2582, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0045, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3751, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0106, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1019, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0393, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-3.5823, -2.1794, -2.5104], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4233, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0079, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6132, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0177, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1022, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0342, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-3.6354, -3.5924, -3.0262], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2379, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0041, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5492, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0147, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1158, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0353, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([1.5382, 7.9194, 1.6746], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6982, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0128, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.4218, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0394, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2616, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0674, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([0.1964, 7.3072, 0.8510], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.7425, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0140, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.3231, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0383, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2469, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0558, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ -6.6593, -10.9725,  -8.5335], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1715, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0026, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4661, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0119, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1517, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0329, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 2.0072, 13.5114,  3.5328], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.0976, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0208, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.9743, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0568, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4150, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0809, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-11.4027, -15.2737, -12.7363], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3120, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0058, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6450, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0178, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1794, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0325, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ -9.6029,  -8.5252, -10.5340], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6744, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0128, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.2233, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0350, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2753, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0490, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-12.8078, -13.1774, -13.2572], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5966, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0111, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.2383, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0336, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4092, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0666, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-11.0759, -15.3087, -15.9375], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5398, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0090, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.4356, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0374, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5297, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0832, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-2.5087,  5.8683,  0.9635], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.2076, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0221, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(2.7057, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0734, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.8700, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1293, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-11.0218, -12.1431, -16.6896], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6364, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0112, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.6906, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0464, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.6211, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0887, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-13.4332, -20.2378, -25.4009], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.3652, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0058, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1854, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0323, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5504, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0741, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ -7.3705,  -9.8556, -16.7093], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5550, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0092, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.7595, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0481, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.8068, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1040, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-16.0009, -11.2416, -15.1759], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5796, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0105, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.4189, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0376, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5458, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0723, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-16.0653, -12.1047, -15.2264], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5124, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0093, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1837, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0312, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.4621, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0588, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([ 9.6492, 19.8639, 22.2739], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.2882, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0233, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(2.7011, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0702, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1010, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1347, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([13.1621, 12.2405, 16.9740], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(1.1640, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0204, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(2.2514, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0578, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.9957, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.1199, grad_fn=<LinalgVectorNormBackward0>)\n",
      "G @ g_tilde=tensor([-12.2255, -18.9225, -20.3128], grad_fn=<MvBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2569, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0042, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.5588, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0150, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.2745, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.norm(p.grad.flatten() - g_tilde_for_p)=tensor(0.0308, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "models = train_with_gem(initialize_model(), tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f6edf16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 250
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_mat = torch.zeros((len(models), len(tasks)))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    for j, task in enumerate(tasks):\n",
    "        acc_mat[i, j] = compute_acc(model, *task)\n",
    "\n",
    "# Plot the acc mat and show the values in the cells.\n",
    "plt.imshow(acc_mat);\n",
    "for i in range(len(models)):\n",
    "    for j in range(len(tasks)):\n",
    "        plt.text(j, i, f\"{acc_mat[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "plt.xticks(range(len(tasks)), [f\"Task {i}\" for i in range(1, len(tasks) + 1)])\n",
    "plt.yticks(range(len(models)), [\"Rand Init\"] + [f\"Post Task {i}\" for i in range(1, len(models))]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
