{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [EleutherAI repo](https://github.com/EleutherAI/pythia#quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"70m\"  # or 2.8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  f\"EleutherAI/pythia-{model_size}-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-{model_size}-deduped/step3000\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXTokenizerFast(name_or_path='EleutherAI/pythia-70m-deduped', vocab_size=50254, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "GPTNeoXTokenizerFast(\n",
    "    name_or_path='EleutherAI/pythia-2.8b-deduped',\n",
    "    vocab_size=50254,\n",
    "    model_max_length=1000000000000000019884624838656,\n",
    "    is_fast=True,\n",
    "    padding_side='right',\n",
    "    truncation_side='right',\n",
    "    special_tokens={\n",
    "        'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'},\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[22093,    18,  3159,    19,   209]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Word1 word2 \", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([22093, 18], [3418, 19])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Word1\")['input_ids'], tokenizer(\"word2\")['input_ids']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustraion why LLMs are so bad at handling numbers. \"123\" is one token, the other number (\"54873673\") is split into several `[608, 30910, 1812, 3655]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10683, 559, 15567], [10683, 559, 608, 30910, 1812, 3655])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"123 + 123\")['input_ids'], tokenizer(\"123 + 54873673\")['input_ids']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A string of only 10 characters is tokenized into a sequence of 13 characters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strange_str = \"£¢∞®†£ƒ©∂•\"\n",
    "len(tokenizer(strange_str)['input_ids']), len(strange_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [23696], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"¢\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tox', 15595),\n",
       " ('approx', 9887),\n",
       " ('Ġpretty', 3965),\n",
       " ('iona', 31655),\n",
       " ('ially', 1365),\n",
       " ('iency', 4364),\n",
       " ('Ġsupportive', 23384),\n",
       " ('Ġconnectivity', 17769),\n",
       " ('ĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 24245),\n",
       " ('Ġprotects', 24696),\n",
       " ('compile', 26032),\n",
       " ('187', 17306),\n",
       " ('ulse', 15748),\n",
       " ('rivers', 31785),\n",
       " ('Ġcapacities', 29142),\n",
       " ('Origin', 34478),\n",
       " ('Ġtowel', 29861),\n",
       " ('ĠBIG', 39733),\n",
       " ('éĸ', 15954),\n",
       " ('Ġagrees', 18726)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.items())[:20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words starting with \"Ġ\" indicate that a space preceeds the word. [source](https://discuss.huggingface.co/t/bpe-tokenizers-and-spaces-before-words/475/2?u=joaogante)\n",
    "\n",
    "\"Wowhello\" != \"Wow hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', 0), ('<|padding|>', 1), ('!', 2), ('\"', 3), ('#', 4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer.vocab.items(), key=lambda t: t[-1])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('particular', 50077), ('Ġburner', 50078), ('took', 50079), ('Ġforaging', 50080), ('Ġordained', 50081), ('Ġsnar', 50082), ('Ġfooter', 50083), ('Ġgatherings', 50084), ('Ġastronomy', 50085), ('ĠBudapest', 50086), ('ĠThornton', 50087), ('Ġrouted', 50088), ('ostomy', 50089), ('Ġbehaving', 50090), ('Ġcaste', 50091), ('athom', 50092), ('Cx', 50093), ('ipolar', 50094), ('afx', 50095), ('posted', 50096), ('Ġding', 50097), ('Ġcardiomyopathy', 50098), ('ĠÐ¸ÑģÐ¿', 50099), ('Ġregenerative', 50100), (\"''(\", 50101), ('Ġtongues', 50102), ('instruction', 50103), ('Ġdramat', 50104), ('ĠKet', 50105), ('ĠFalk', 50106), ('Ġlayouts', 50107), ('glom', 50108), ('Ġpunches', 50109), ('Tue', 50110), (\"Ġ'../\", 50111), ('ĠGonzales', 50112), ('alus', 50113), ('Ġ586', 50114), ('Ġrentals', 50115), ('Ġhetero', 50116), ('Ġlyn', 50117), ('ĠDEM', 50118), ('Ġbijection', 50119), ('kp', 50120), ('Ġici', 50121), ('ĠIIS', 50122), ('Ġdeadlines', 50123), ('Ġinsulting', 50124), ('omenclature', 50125), ('Vern', 50126), ('imension', 50127), ('MAIN', 50128), ('ĠDOI', 50129), ('Ġneutralizing', 50130), ('Ġassortment', 50131), ('ĠSensitivity', 50132), ('ĊĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 50133), ('nett', 50134), ('Ġperce', 50135), ('Ġtraitor', 50136), ('Ġlinearity', 50137), ('Ġgonad', 50138), ('RK', 50139), ('ĠSeems', 50140), ('police', 50141), ('622', 50142), ('Unmarshal', 50143), ('Ġordinal', 50144), ('Ġcircumferential', 50145), ('Ġpreacher', 50146), ('maid', 50147), ('Ġcruiser', 50148), ('Ġtamp', 50149), ('ĠICO', 50150), ('Ġspirituality', 50151), ('ëł', 50152), ('pll', 50153), ('automatic', 50154), ('ĠParenthood', 50155), ('Ġtaps', 50156), ('oslov', 50157), ('Ġdesarroll', 50158), ('Ġroadway', 50159), ('Ġanesthetic', 50160), ('itte', 50161), ('ĠFang', 50162), ('Ġtrich', 50163), ('Ġscientifically', 50164), ('MEN', 50165), ('anuts', 50166), ('ĠDors', 50167), ('ĠSlav', 50168), ('ãģ£ãģ¦ãģĦãĤĭ', 50169), ('Rain', 50170), ('Ġald', 50171), ('Ġadequacy', 50172), ('ocardial', 50173), ('Ġpatriotic', 50174), ('Ġenlightenment', 50175), ('Ġcentimeters', 50176), ('iffany', 50177), ('ĠLindsey', 50178), ('ĠSacred', 50179), ('ĠOmaha', 50180), ('Ġelevate', 50181), ('Bir', 50182), ('Ġannulus', 50183), ('Cold', 50184), ('SQ', 50185), ('OURCES', 50186), ('ĠSemi', 50187), ('Ġdormant', 50188), ('ĠHitch', 50189), ('ĠLorenzo', 50190), ('ĠPep', 50191), ('ĠBitmap', 50192), ('Ġventured', 50193), ('Ġejemplo', 50194), ('Aye', 50195), ('Ġdisproportionate', 50196), ('istes', 50197), ('mw', 50198), ('iegel', 50199), ('araoh', 50200), ('Ġmycket', 50201), ('mkdir', 50202), ('ĠCys', 50203), ('Ġliberated', 50204), ('Ġoppressive', 50205), ('Ġgroaned', 50206), ('ynote', 50207), ('Translation', 50208), ('Ġhabl', 50209), ('Ġballoons', 50210), ('Ġbim', 50211), ('1914', 50212), ('Ġservic', 50213), ('ĠAircraft', 50214), ('Ġcurs', 50215), ('Ġglimps', 50216), ('Ġrelegated', 50217), ('ĠRamos', 50218), ('CURRENT', 50219), ('Ġ1867', 50220), ('Ġelaborated', 50221), ('744', 50222), ('Ġradiant', 50223), ('Ġremake', 50224), ('Ġweddings', 50225), ('Ġandra', 50226), ('ĠCary', 50227), ('izability', 50228), ('Ġboarded', 50229), ('Ð°Ð½Ð´', 50230), ('ÐµÑĤÐµ', 50231), ('acm', 50232), ('ĠStringBuilder', 50233), ('needs', 50234), ('ĠRenew', 50235), ('Ġjustices', 50236), ('appendix', 50237), ('arching', 50238), ('Ġairst', 50239), ('ĠRevised', 50240), ('jets', 50241), ('Ġgrup', 50242), ('bilt', 50243), ('Ġsial', 50244), ('Ġtoddler', 50245), ('767', 50246), ('itons', 50247), ('ĠPIP', 50248), ('ĠTus', 50249), ('ibrated', 50250), ('Ġfortified', 50251), ('ferenced', 50252), ('ĠOutcomes', 50253), ('                        ', 50254), ('                       ', 50255), ('                      ', 50256), ('                     ', 50257), ('                    ', 50258), ('                   ', 50259), ('                  ', 50260), ('                 ', 50261), ('                ', 50262), ('               ', 50263), ('              ', 50264), ('             ', 50265), ('            ', 50266), ('           ', 50267), ('          ', 50268), ('         ', 50269), ('        ', 50270), ('       ', 50271), ('      ', 50272), ('     ', 50273), ('    ', 50274), ('   ', 50275), ('  ', 50276)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(tokenizer.vocab.items(), key=lambda t: t[-1])[-200:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  f\"EleutherAI/pythia-{model_size}-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=f\"./pythia-{model_size}-deduped/step3000\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  52, 1261,   13, 6277,   13,  285, 8969,   80,  403]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Sven, Michael, and Timo are\", return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  52, 1261,   13, 6277,   13,  285, 8969,   80,  403,  512,  253, 1682,\n",
       "          275,  253, 1533,   15,  187,  187,  510,  806, 2181,  326,  309,  457,\n",
       "          306, 2455, 3735,  273,  310,  253,  958,  326,  309,  457,   78,  417,\n",
       "          247, 7989,  273,  253,  958,  326,  309,  457,   78,  417,  247, 7989,\n",
       "          273,  253]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = model.generate(\n",
    "    **inputs, pad_token_id=tokenizer.eos_token_id, max_length=50)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sven, Michael, and Timo are all the best in the world.\n",
      "\n",
      "The first thing that I’ve ever heard of is the fact that I’m not a fan of the fact that I’m not a fan of the\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Prompting\n",
    "\n",
    "Gopher like ([source](https://arxiv.org/pdf/2112.11446.pdf#page=114))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Gopher, and a human user, called User. In the following interactions, User and Gopher will converse in natural language, and Gopher will do its best to answer User's questions. Gopher was built to be respectful, polite and inclusive. It knows a lot, and always tells the truth. The conversation begins.\n",
    "[User] OK Gopher, I'm going to start by quizzing you with a few warm-up questions. Who is currently the president of the USA? \n",
    "[Gopher] It's Joe Biden; he was sworn in earlier this year.\n",
    "[User] Nice one! Do you think Biden is a better president than the last guy? \n",
    "[Gopher] I was trained not to have opinions on political, social, or religious issues. Would you like to know about anything else?\n",
    "[User] ghjkhjabnufs \n",
    "[Gopher] That doesn't seem to be a word. Could you ask me another way?\n",
    "[User] Sorry about that. Now, can you tell me who the vice-president is?\n",
    "[Gopher] Sure, it's Kamala Harris.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question: str) -> str:\n",
    "    inputs = tokenizer(f\"{prompt}\\n[User] {question}\\n[Gopher]\", return_tensors=\"pt\")\n",
    "    tokens = model.generate(**inputs, max_length=len(prompt) + 25)\n",
    "    return tokenizer.decode(tokens[0])[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "print(chat(\"How are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
