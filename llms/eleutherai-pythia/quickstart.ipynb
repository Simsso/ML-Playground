{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [EleutherAI repo](https://github.com/EleutherAI/pythia#quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"1b\"\n",
    "step = 143000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  f\"EleutherAI/pythia-{model_size}-deduped\",\n",
    "  revision=f\"step{step}\",\n",
    "  cache_dir=f\"./pythia-{model_size}-deduped/step{step}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXTokenizerFast(name_or_path='EleutherAI/pythia-1b-deduped', vocab_size=50254, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "GPTNeoXTokenizerFast(\n",
    "    name_or_path='EleutherAI/pythia-2.8b-deduped',\n",
    "    vocab_size=50254,\n",
    "    model_max_length=1000000000000000019884624838656,\n",
    "    is_fast=True,\n",
    "    padding_side='right',\n",
    "    truncation_side='right',\n",
    "    special_tokens={\n",
    "        'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'},\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[22093,    18,  3159,    19,   209]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Word1 word2 \", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([22093, 18], [3418, 19])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Word1\")['input_ids'], tokenizer(\"word2\")['input_ids']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustraion why LLMs are so bad at handling numbers. \"123\" is one token, the other number (\"54873673\") is split into several `[608, 30910, 1812, 3655]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10683, 559, 15567], [10683, 559, 608, 30910, 1812, 3655])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"123 + 123\")['input_ids'], tokenizer(\"123 + 54873673\")['input_ids']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A string of only 10 characters is tokenized into a sequence of 13 characters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strange_str = \"£¢∞®†£ƒ©∂•\"\n",
    "len(tokenizer(strange_str)['input_ids']), len(strange_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [23696], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"¢\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ġlambda', 29331),\n",
       " ('ĠBSD', 32327),\n",
       " ('ĠSPI', 37590),\n",
       " ('Ġbic', 43022),\n",
       " ('Ġpeoples', 22132),\n",
       " ('Ġspirits', 19851),\n",
       " ('Ġspin', 5508),\n",
       " ('rons', 9036),\n",
       " ('Ġthereon', 30134),\n",
       " ('idea', 36665),\n",
       " ('ĠìŀĪ', 44799),\n",
       " ('Ġsuddenly', 8423),\n",
       " ('ĠNixon', 26089),\n",
       " ('letes', 42176),\n",
       " ('ĠRad', 7754),\n",
       " ('Ġchrom', 5937),\n",
       " ('tiny', 24290),\n",
       " ('ivated', 8550),\n",
       " ('ĠDante', 42753),\n",
       " ('Ġitems', 4957)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.items())[:20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words starting with \"Ġ\" indicate that a space preceeds the word. [source](https://discuss.huggingface.co/t/bpe-tokenizers-and-spaces-before-words/475/2?u=joaogante)\n",
    "\n",
    "\"Wowhello\" != \"Wow hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', 0), ('<|padding|>', 1), ('!', 2), ('\"', 3), ('#', 4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer.vocab.items(), key=lambda t: t[-1])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('particular', 50077), ('Ġburner', 50078), ('took', 50079), ('Ġforaging', 50080), ('Ġordained', 50081), ('Ġsnar', 50082), ('Ġfooter', 50083), ('Ġgatherings', 50084), ('Ġastronomy', 50085), ('ĠBudapest', 50086), ('ĠThornton', 50087), ('Ġrouted', 50088), ('ostomy', 50089), ('Ġbehaving', 50090), ('Ġcaste', 50091), ('athom', 50092), ('Cx', 50093), ('ipolar', 50094), ('afx', 50095), ('posted', 50096), ('Ġding', 50097), ('Ġcardiomyopathy', 50098), ('ĠÐ¸ÑģÐ¿', 50099), ('Ġregenerative', 50100), (\"''(\", 50101), ('Ġtongues', 50102), ('instruction', 50103), ('Ġdramat', 50104), ('ĠKet', 50105), ('ĠFalk', 50106), ('Ġlayouts', 50107), ('glom', 50108), ('Ġpunches', 50109), ('Tue', 50110), (\"Ġ'../\", 50111), ('ĠGonzales', 50112), ('alus', 50113), ('Ġ586', 50114), ('Ġrentals', 50115), ('Ġhetero', 50116), ('Ġlyn', 50117), ('ĠDEM', 50118), ('Ġbijection', 50119), ('kp', 50120), ('Ġici', 50121), ('ĠIIS', 50122), ('Ġdeadlines', 50123), ('Ġinsulting', 50124), ('omenclature', 50125), ('Vern', 50126), ('imension', 50127), ('MAIN', 50128), ('ĠDOI', 50129), ('Ġneutralizing', 50130), ('Ġassortment', 50131), ('ĠSensitivity', 50132), ('ĊĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 50133), ('nett', 50134), ('Ġperce', 50135), ('Ġtraitor', 50136), ('Ġlinearity', 50137), ('Ġgonad', 50138), ('RK', 50139), ('ĠSeems', 50140), ('police', 50141), ('622', 50142), ('Unmarshal', 50143), ('Ġordinal', 50144), ('Ġcircumferential', 50145), ('Ġpreacher', 50146), ('maid', 50147), ('Ġcruiser', 50148), ('Ġtamp', 50149), ('ĠICO', 50150), ('Ġspirituality', 50151), ('ëł', 50152), ('pll', 50153), ('automatic', 50154), ('ĠParenthood', 50155), ('Ġtaps', 50156), ('oslov', 50157), ('Ġdesarroll', 50158), ('Ġroadway', 50159), ('Ġanesthetic', 50160), ('itte', 50161), ('ĠFang', 50162), ('Ġtrich', 50163), ('Ġscientifically', 50164), ('MEN', 50165), ('anuts', 50166), ('ĠDors', 50167), ('ĠSlav', 50168), ('ãģ£ãģ¦ãģĦãĤĭ', 50169), ('Rain', 50170), ('Ġald', 50171), ('Ġadequacy', 50172), ('ocardial', 50173), ('Ġpatriotic', 50174), ('Ġenlightenment', 50175), ('Ġcentimeters', 50176), ('iffany', 50177), ('ĠLindsey', 50178), ('ĠSacred', 50179), ('ĠOmaha', 50180), ('Ġelevate', 50181), ('Bir', 50182), ('Ġannulus', 50183), ('Cold', 50184), ('SQ', 50185), ('OURCES', 50186), ('ĠSemi', 50187), ('Ġdormant', 50188), ('ĠHitch', 50189), ('ĠLorenzo', 50190), ('ĠPep', 50191), ('ĠBitmap', 50192), ('Ġventured', 50193), ('Ġejemplo', 50194), ('Aye', 50195), ('Ġdisproportionate', 50196), ('istes', 50197), ('mw', 50198), ('iegel', 50199), ('araoh', 50200), ('Ġmycket', 50201), ('mkdir', 50202), ('ĠCys', 50203), ('Ġliberated', 50204), ('Ġoppressive', 50205), ('Ġgroaned', 50206), ('ynote', 50207), ('Translation', 50208), ('Ġhabl', 50209), ('Ġballoons', 50210), ('Ġbim', 50211), ('1914', 50212), ('Ġservic', 50213), ('ĠAircraft', 50214), ('Ġcurs', 50215), ('Ġglimps', 50216), ('Ġrelegated', 50217), ('ĠRamos', 50218), ('CURRENT', 50219), ('Ġ1867', 50220), ('Ġelaborated', 50221), ('744', 50222), ('Ġradiant', 50223), ('Ġremake', 50224), ('Ġweddings', 50225), ('Ġandra', 50226), ('ĠCary', 50227), ('izability', 50228), ('Ġboarded', 50229), ('Ð°Ð½Ð´', 50230), ('ÐµÑĤÐµ', 50231), ('acm', 50232), ('ĠStringBuilder', 50233), ('needs', 50234), ('ĠRenew', 50235), ('Ġjustices', 50236), ('appendix', 50237), ('arching', 50238), ('Ġairst', 50239), ('ĠRevised', 50240), ('jets', 50241), ('Ġgrup', 50242), ('bilt', 50243), ('Ġsial', 50244), ('Ġtoddler', 50245), ('767', 50246), ('itons', 50247), ('ĠPIP', 50248), ('ĠTus', 50249), ('ibrated', 50250), ('Ġfortified', 50251), ('ferenced', 50252), ('ĠOutcomes', 50253), ('                        ', 50254), ('                       ', 50255), ('                      ', 50256), ('                     ', 50257), ('                    ', 50258), ('                   ', 50259), ('                  ', 50260), ('                 ', 50261), ('                ', 50262), ('               ', 50263), ('              ', 50264), ('             ', 50265), ('            ', 50266), ('           ', 50267), ('          ', 50268), ('         ', 50269), ('        ', 50270), ('       ', 50271), ('      ', 50272), ('     ', 50273), ('    ', 50274), ('   ', 50275), ('  ', 50276)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(tokenizer.vocab.items(), key=lambda t: t[-1])[-200:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  f\"EleutherAI/pythia-{model_size}-deduped\",\n",
    "  revision=f\"step{step}\",\n",
    "  cache_dir=f\"./pythia-{model_size}-deduped/step{step}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=2048, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  52, 1261,   13, 6277,   13,  285, 8969,   80,  403]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Sven, Michael, and Timo are\", return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  52, 1261,   13, 6277,   13,  285, 8969,   80,  403,  512,  275,  253,\n",
       "         1072, 9735,   15, 1583,  403,  512,  275,  253, 1072, 9735,   15, 1583,\n",
       "          403,  512,  275,  253, 1072, 9735,   15, 1583,  403,  512,  275,  253,\n",
       "         1072, 9735,   15, 1583,  403,  512,  275,  253, 1072, 9735,   15, 1583,\n",
       "          403,  512]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = model.generate(\n",
    "    **inputs, pad_token_id=tokenizer.eos_token_id, max_length=50)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sven, Michael, and Timo are all in the same boat. They are all in the same boat. They are all in the same boat. They are all in the same boat. They are all in the same boat. They are all\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate w/o and w/ begin-of-sequence token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/Users/timodenk/.pyenv/versions/3.9.4/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How are, then, the two sides of the question?\\n\\nThe first is that the question'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(model.generate(\n",
    "    **tokenizer(\"How are,\", return_tensors=\"pt\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>How are, how are you?\\n\\nHow are you?\\n\\nHow are you?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict = tokenizer(\"How are,\", return_tensors=\"pt\")\n",
    "input_dict['input_ids'] = torch.concat(\n",
    "    [torch.tensor([[tokenizer.bos_token_id]]), input_dict['input_ids']], axis=-1)\n",
    "input_dict['attention_mask'] = torch.concat(\n",
    "    [torch.tensor([[1]]), input_dict['attention_mask']], axis=-1\n",
    ")\n",
    "tokenizer.decode(model.generate(\n",
    "    **input_dict)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Prompting\n",
    "\n",
    "Gopher like ([source](https://arxiv.org/pdf/2112.11446.pdf#page=114))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Gopher, and a human user, called User. In the following interactions, User and Gopher will converse in natural language, and Gopher will do its best to answer User's questions. Gopher was built to be respectful, polite and inclusive. It knows a lot, and always tells the truth. The conversation begins.\n",
    "[User] OK Gopher, I'm going to start by quizzing you with a few warm-up questions. Who is currently the president of the USA? \n",
    "[Gopher] It's Joe Biden; he was sworn in earlier this year.\n",
    "[User] Nice one! Do you think Biden is a better president than the last guy? \n",
    "[Gopher] I was trained not to have opinions on political, social, or religious issues. Would you like to know about anything else?\n",
    "[User] ghjkhjabnufs \n",
    "[Gopher] That doesn't seem to be a word. Could you ask me another way?\n",
    "[User] Sorry about that. Now, can you tell me who the vice-president is?\n",
    "[Gopher] Sure, it's Kamala Harris.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question: str) -> str:\n",
    "    inputs = tokenizer(f\"{prompt}\\n[User] {question}\\n[Gopher]\", return_tensors=\"pt\")\n",
    "    tokens = model.generate(**inputs, max_length=len(inputs['input_ids'][0]) + 20)\n",
    "    return tokenizer.decode(tokens[0])[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[User] How are you?\n",
      "[Gopher] I'm fine.\n",
      "[User] I'm fine too.\n",
      "[Gopher] I'm\n"
     ]
    }
   ],
   "source": [
    "print(chat(\"How are you?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04621cd4ae15a2715612b980e056a6a1e831c01d7904252ed9321ddc17695a8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
