{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL for LLMs\n",
    "\n",
    "## Background: REINFORCE\n",
    "\n",
    "Loss $l(f_\\theta(x)) = \\left(10 - \\lvert f_\\theta(x)\\rvert\\right)^2$ is non-differentiable, $x$ is a sentence, $\\theta$ are model weights. Objective:\n",
    "\n",
    "$$\\min_\\theta l(f_\\theta(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't optimize the objective directly, because $l$ is non-differentiable.\n",
    "Instead, we get the expectation of the loss by sampling from a distribution which we _can_ optimize.\n",
    "\n",
    "Now we seek another objective that's similar\n",
    "\n",
    "\\begin{align}\n",
    "&\\min_\\mu\\int l(f_\\theta(x))\\ p(\\theta;\\mu)\\ d\\theta\\\\\n",
    "=&E_{p(\\theta;\\mu)} [l(f_\\theta(x))]\\\\\n",
    "\\approx&\\frac{1}{N} \\sum_{n=1}^Nl(f_{\\theta_n}(x)) \\quad \\text{with}\\quad\\theta_n\\sim p(\\theta;\\mu) \\quad \\text{(MC estimation)}\n",
    "\\end{align}\n",
    "\n",
    "$p(\\theta;\\mu)$ could be something like $\\mathcal{N}(\\mu,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize for $\\mu$:\n",
    "\n",
    "\\begin{align}\n",
    "&\\nabla_\\mu\\int l(f_\\theta(x))\\ p(\\theta;\\mu)\\ d\\theta\\\\\n",
    "=&\\int \\underbrace{l(f_\\theta(x))}_\\text{net non diff.}\\ \\nabla_\\mu p(\\theta;\\mu)\\ d\\theta\\\\\n",
    "\\end{align}\n",
    "\n",
    "---\n",
    "\n",
    "REINFORCE gradient estimate / score function estimate:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_x\\ln(x) =& \\frac{1}{x}\\nabla_xx\\\\\n",
    "\\underbrace{\\nabla_\\mu\\ln(p(\\theta;\\mu))}_{\\text{score function}} =& \\frac{1}{p(\\theta;\\mu)}\\nabla_\\mu p(\\theta;\\mu)\\quad\\vert \\cdot p(\\theta;\\mu)\\\\\n",
    "\\nabla_\\mu\\ln(p(\\theta;\\mu)) \\cdot p(\\theta;\\mu) =& \\nabla_\\mu p(\\theta;\\mu)\\\\\n",
    "\\end{align}\n",
    "\n",
    "---\n",
    "\n",
    "\\begin{align}\n",
    "&\\int l(f_\\theta(x))\\ \\nabla_\\mu p(\\theta;\\mu)\\ d\\theta\\\\\n",
    "=&\\int l(f_\\theta(x))\\ \\nabla_\\mu\\ln(p(\\theta;\\mu)) \\cdot p(\\theta;\\mu)\\ d\\theta\\\\\n",
    "\\approx&\\frac{1}{n}\\sum_i^n l(f_{\\theta_i}(x))\\ \\nabla_\\mu\\ln(p(\\theta_i;\\mu))\\quad\\theta_i\\sim p(\\theta;\\mu)\\\\\n",
    "=&\n",
    "\\end{align}\n",
    "\n",
    "Assume $p = \\mathcal{N}(\\theta;\\mu,I)$ (we go through the 1D case here):\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_\\mu\\ln(p(\\theta_i;\\mu)) =& \\nabla_\\mu\\ln\\left( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left( -\\frac{(\\theta - \\mu)^2}{2\\sigma^2} \\right)\\right)\\\\\n",
    "=& \\underbrace{\\nabla_\\mu \\ln\\left( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\right)}_{=0} - \\nabla_\\mu \\frac{(\\theta - \\mu)^2}{2\\sigma^2}\\\\\n",
    "=& -\\frac{1}{2\\sigma^2} 2 (\\theta-\\mu) \\cdot (-1)\\\\\n",
    "=& (\\theta-\\mu)\n",
    "\\end{align*}\n",
    "\n",
    "Does REINFORCE work for LLMs? Concern is that sampling from $p(\\theta_i;\\mu)$ may not work for a $\\theta$ which is high dim (e.g., 70M or 175B in the real world)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient update with (3)\n",
    "\n",
    "\\begin{align}\n",
    "\\mu^{(t+1)} \\gets& \\mu^{(t)} - \\alpha \\underbrace{\\frac{1}{n}\\sum_i^n l(f_{\\theta_i}(x))\\ \\nabla_\\mu\\ln(p(\\theta_i;\\mu))\\quad\\theta_i\\sim p(\\theta;\\mu)}_{(3)} \\\\\n",
    "\\gets& \\mu^{(t)} - \\alpha \\frac{1}{n}\\sum_i^n l(f_{\\theta_i}(x))\\ (\\theta_i - \\mu)\\quad\\theta_i\\sim p(\\theta;\\mu)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([150, 4]), torch.Size([150]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = load_iris(return_X_y=True)\n",
    "x, y = torch.tensor(x).float(), torch.tensor(y)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\min_w \\quad \\int \\operatorname{acc}(y, y_\\text{true}) p(y;w)\\ dy\\\\\n",
    "\\min_w \\quad \\sum_{i} \\operatorname{acc}(y_i, y_\\text{true}) p(y(x);w)_i\\\\\n",
    "\\end{align}\n",
    "\n",
    "We could use (2) if we didn't want to sample. For a discrete RV with just four possible outcomes as we have it here we could loop, for an LLM the # of outcomes is much larger. Below we do the case where we sample from the discrete RM.\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_w \\int \\operatorname{acc}(y, y_\\text{true}) p(y;w)\\ dy =& \\int \\operatorname{acc}(y, y_\\text{true})\\ \\nabla_w p(y;w)\\ dy \\\\\n",
    "=& \\int \\operatorname{acc}(y, y_\\text{true})\\ \\nabla_w\\ln(p(y;w)) \\cdot p(y;w)\\ dy \\quad \\vert \\quad \\text{for a given }x\\\\\n",
    "\\approx& \\frac{1}{N}\\sum_{i=1}^N \\operatorname{acc}(y_n(x), y_\\text{true})\\ \\nabla_w\\ln(p(y_n(x);w)) \\quad \\text{with} \\quad y_n\\sim p(y(x);w)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "&\\nabla_w \\quad \\sum_{i} \\operatorname{acc}(y_i, y_\\text{true})\\ p(y(x);w)_i\\\\\n",
    "=&\\sum_{i} \\operatorname{acc}(y_i, y_\\text{true})\\ \\nabla_w p(y(x);w)_i\\\\\n",
    "=&\\sum_{i} \\operatorname{acc}(y_i, y_\\text{true})\\ \\nabla_w \\log p(y(x);w)_i \\cdot p(y(x);w)\\\\\n",
    "=&\\sum_{i=1}^N \\operatorname{acc}(y_i, y_\\text{true})\\ \\nabla_w \\underbrace{\\log p(y(x);w)_i}_{\\text{log\\_probs[drawn\\_class]}} \\quad y_i \\sim p(y(x);w)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=x.shape[1], out_features=4),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=4, out_features=3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=3, out_features=4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "rewards = []\n",
    "\n",
    "num_epochs = 10000\n",
    "num_mc_samples = 1\n",
    "for i in range(num_epochs):\n",
    "    logits = net(x)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    drawn_class = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "    reward = (drawn_class == y).float()\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    selected_log_probs = log_probs.index_select(dim=-1, index=drawn_class)\n",
    "\n",
    "    net.zero_grad()\n",
    "    loss = 1 / num_mc_samples * (-reward * selected_log_probs).mean()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    rewards.append(reward.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.asarray(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3107), tensor(0.3371))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10].mean(), rewards[-50:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: PPO\n",
    "\n",
    "Importance sampling: \n",
    "\n",
    "\\begin{align}\n",
    "&\\int f(x)p(x)\\ dx = E_p[f(x)]\\\\\n",
    "=&\\int f(x)p(x) \\cdot 1\\ dx\\\\\n",
    "=&\\int f(x)p(x) \\cdot \\frac{q(x)}{q(x)}\\ dx\\\\\n",
    "=&\\int f(x)q(x) \\cdot \\frac{p(x)}{q(x)}\\ dx = E_q\\left[f(x)\\frac{p(x)}{q(x)}\\right]\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite after looking into `grpo_loss` in [this file](https://github.com/aburkov/theLMbook/blob/main/GRPO.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Symbol | Meaning |\n",
    "| ---:| --- |\n",
    "| $\\phi$ | model weights |\n",
    "| $x,y$ | model input (prompt) and sampled outputs |\n",
    "| $r(x,y)$ | reward |\n",
    "| $\\pi_\\phi^\\text{RL}$ | policy (model) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to find / our objective:\n",
    "\n",
    "\\begin{align}\n",
    "&\\displaystyle \\nabla_\\phi E_{\\pi_\\phi^\\text{RL}(x,y)}\\left[r(x,y)\\right]\\\\\n",
    "=&\\nabla_\\phi\\int r(x,y)\\ \\pi_\\phi^\\text{RL}(x,y)\\ dxdy\\\\\n",
    "=&\\int r(x,y)\\ \\nabla_\\phi \\pi_\\phi^\\text{RL}(x,y)\\ dxdy\\\\\n",
    "=&\\int r(x,y)\\ \\nabla_\\phi \\ln \\pi_\\phi^\\text{RL}(x,y) \\cdot \\pi_\\phi^\\text{RL}(x,y)\\ dxdy\\\\\n",
    "\\approx&\\frac{1}{N}\\sum_n^N r(x,y)\\  \\nabla_\\phi \\ln \\pi_\\phi^\\text{RL}(x_n,y_n) & \\text{with } (x_n,y_n) \\sim \\pi_\\phi^\\text{RL}(x,y)\\\\\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
