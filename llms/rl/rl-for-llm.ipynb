{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL for LLMs\n",
    "\n",
    "## Background: REINFORCE\n",
    "\n",
    "Loss $l(f_\\theta(x)) = \\left(10 - \\lvert f_\\theta(x)\\rvert\\right)^2$ is non-differentiable, $x$ is a sentence, $\\theta$ are model weights. Objective:\n",
    "\n",
    "$$\\min_\\theta l(f_\\theta(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we seek another objective that's similar\n",
    "\n",
    "\\begin{align}\n",
    "&\\min_\\mu\\int l(f_\\theta(x))\\ p(\\theta;\\mu)\\ d\\theta\\\\\n",
    "=&E_{p(\\theta;\\mu)} [f_\\theta(x)]\n",
    "\\end{align}\n",
    "\n",
    "$p(\\theta;\\mu)$ could be something like $\\mathcal{N}(\\mu,1)$\n",
    "\n",
    "To optimize for $\\mu$:\n",
    "\n",
    "\\begin{align}\n",
    "&\\nabla_\\mu\\int l(f_\\theta(x))\\ p(\\theta;\\mu)\\ d\\theta\\\\\n",
    "=&\\int \\underbrace{l(f_\\theta(x))}_\\text{net non diff.}\\ \\nabla_\\mu p(\\theta;\\mu)\\ d\\theta\\\\\n",
    "\\end{align}\n",
    "\n",
    "---\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_x\\ln(x) =& \\frac{1}{x}\\nabla_xx\\\\\n",
    "\\nabla_\\mu\\ln(p(\\theta;\\mu)) =& \\frac{1}{p(\\theta;\\mu)}\\nabla_\\mu p(\\theta;\\mu)\\quad\\vert \\cdot p(\\theta;\\mu)\\\\\n",
    "\\nabla_\\mu\\ln(p(\\theta;\\mu)) \\cdot p(\\theta;\\mu) =& \\nabla_\\mu p(\\theta;\\mu)\\\\\n",
    "\\end{align}\n",
    "\n",
    "---\n",
    "\n",
    "\\begin{align}\n",
    "&\\int l(f_\\theta(x))\\ \\nabla_\\mu p(\\theta;\\mu)\\ d\\theta\\\\\n",
    "=&\\int l(f_\\theta(x))\\ \\nabla_\\mu\\ln(p(\\theta;\\mu)) \\cdot p(\\theta;\\mu)\\ d\\theta\\\\\n",
    "\\approx&\\frac{1}{n}\\sum_i^n l(f_{\\theta_i}(x))\\ \\nabla_\\mu\\ln(p(\\theta_i;\\mu))\\quad\\theta_i\\sim p(\\theta;\\mu)\n",
    "\\end{align}\n",
    "\n",
    "Does REINFORCE work for LLMs? Concern is that sampling from $p(\\theta_i;\\mu)$ may not work for a $\\theta$ which is high dim (e.g., 70M or 175B in the real world).\n",
    "\n",
    "## Background: PPO\n",
    "\n",
    "Importance sampling: \n",
    "\n",
    "\\begin{align}\n",
    "&\\int f(x)p(x)\\ dx = E_p[f(x)]\\\\\n",
    "=&\\int f(x)p(x) \\cdot 1\\ dx\\\\\n",
    "=&\\int f(x)p(x) \\cdot \\frac{q(x)}{q(x)}\\ dx\\\\\n",
    "=&\\int f(x)q(x) \\cdot \\frac{p(x)}{q(x)}\\ dx = E_q\\left[f(x)\\frac{p(x)}{q(x)}\\right]\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite after looking into `grpo_loss` in [this file](https://github.com/aburkov/theLMbook/blob/main/GRPO.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Symbol | Meaning |\n",
    "| ---:| --- |\n",
    "| $\\phi$ | model weights |\n",
    "| $x,y$ | model input (prompt) and sampled outputs |\n",
    "| $r(x,y)$ | reward |\n",
    "| $\\pi_\\phi^\\text{RL}$ | policy (model) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to find / our objective:\n",
    "\n",
    "\\begin{align}\n",
    "&\\displaystyle \\nabla_\\phi E_{\\pi_\\phi^\\text{RL}(x,y)}\\left[r(x,y)\\right]\\\\\n",
    "=&\\nabla_\\phi\\int r(x,y)\\ \\pi_\\phi^\\text{RL}(x,y)\\ dxdy\\\\\n",
    "=&\\int r(x,y)\\ \\nabla_\\phi \\pi_\\phi^\\text{RL}(x,y)\\ dxdy\\\\\n",
    "=&\\int r(x,y)\\ \\nabla_\\phi \\ln \\pi_\\phi^\\text{RL}(x,y) \\cdot \\pi_\\phi^\\text{RL}(x,y)\\ dxdy\\\\\n",
    "\\approx&\\frac{1}{N}\\sum_n^N r(x,y)\\  \\nabla_\\phi \\ln \\pi_\\phi^\\text{RL}(x_n,y_n) & \\text{with } (x_n,y_n) \\sim \\pi_\\phi^\\text{RL}(x,y)\\\\\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
