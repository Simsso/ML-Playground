{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"70m\"  # or 2.8b\n",
    "revision = \"step143000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timodenk/dev/ml-tinkering/llms/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import functools\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  f\"EleutherAI/pythia-{model_size}-deduped\",\n",
    "  revision=revision,\n",
    "  cache_dir=f\"./pythia-{model_size}-deduped/{revision}\",\n",
    "  padding_side='left',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = GPTNeoXForCausalLM.from_pretrained(\n",
    "  f\"EleutherAI/pythia-{model_size}-deduped\",\n",
    "  revision=revision,\n",
    "  cache_dir=f\"./pythia-{model_size}-deduped/{revision}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(lm, text: str) -> str:\n",
    "    tokens = lm.generate(\n",
    "        **tokenizer(text, return_tensors=\"pt\"),\n",
    "        pad_token_id=tokenizer.eos_token_id, max_length=30)\n",
    "    return tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi there, how can I get the data from the database?\\n\\nA:\\n\\nI'm not sure what you mean.  I'm\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(lm, \"Hi there, how\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tell me a joke!\\n\\nI'm not sure what to do with the words.\\n\\nI'm not sure what to do with the words\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(lm, \"Tell me a joke!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_contains_one_of(sequence: str, words: set[str]) -> float:\n",
    "    sequence_words = set(word.lower() for word in sequence.split())\n",
    "    return len(sequence_words & words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_length(sample: str, target_len: int) -> float:\n",
    "    return -abs(len(sample) - target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "@functools.cache\n",
    "def bert_sentence_embedding(sentence: str) -> torch.Tensor:    \n",
    "    inputs = bert_tokenizer(\n",
    "        sentence, return_tensors='pt', padding=True, truncation=True\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    return embeddings.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_emb_dist(t1: str, t2: str) -> float:\n",
    "    t1_emb = bert_sentence_embedding(t1)\n",
    "    t2_emb = bert_sentence_embedding(t2)\n",
    "\n",
    "    return -torch.norm(t1_emb - t2_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-8.4843), tensor(-4.2851), tensor(-10.2245)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    bert_emb_dist(\"How are you?\", \"What's up?\"),\n",
    "    bert_emb_dist(\"How are you?\", \"How are you?!\"),\n",
    "    bert_emb_dist(\"How are you?\", \"Beethoven was a great composer.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2054,\n",
       " ['Now explain it to a dog',\n",
       "  'Is SEO relevant in 2023?',\n",
       "  \"Don't care, didn't ask!\",\n",
       "  \"I'm sorry\",\n",
       "  'No I just wanted to thank you.'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAssistant dataset\n",
    "\n",
    "def is_capitalized(s: str) -> bool:\n",
    "    return s[:1].isupper() and s[1:] == s[1:].lower()\n",
    "\n",
    "oasst1_data = load_dataset(\"OpenAssistant/oasst1\")\n",
    "oasst1_prompts = [\n",
    "    x['text']\n",
    "    for x in oasst1_data['train']\n",
    "    if 0 < len(x['text']) < 32 and x['lang'] == 'en' and is_capitalized(x['text'][0])\n",
    "]\n",
    "len(oasst1_prompts), oasst1_prompts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_log_probs(tokens, log_probs, prompt_len: int):\n",
    "    \"\"\"Returns the scores of the tokens that were sampled.\n",
    "\n",
    "    tokens: [mc_samples, seq_len (incl. prompt)]\n",
    "    log_probs: [len seq_len (excl. prompt), mc_samples, vocab_size]\n",
    "    \"\"\"\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for mc_sample_idx in range(len(tokens)):\n",
    "        sample_log_probs = []\n",
    "        for token_pos in range(len(tokens[mc_sample_idx, prompt_len:])):\n",
    "            token_idx = tokens[mc_sample_idx][token_pos + prompt_len]\n",
    "            log_prob = log_probs[token_pos][mc_sample_idx][token_idx]\n",
    "            sample_log_probs.append(log_prob)\n",
    "        output.append(torch.stack(sample_log_probs))\n",
    "\n",
    "    return output  # list[tensor] with shape [mc_samples, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_mc_samples(prompt, lm, num_samples, temperature=1.0, max_length=30):\n",
    "    \"\"\"Draws MC samples for a given prompt.\"\"\"\n",
    "    prompt_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    prompt_input_ids = prompt_input['input_ids']\n",
    "    prompt_attention_mask = prompt_input['attention_mask']\n",
    "\n",
    "    outputs = lm.generate(\n",
    "        input_ids=prompt_input_ids.repeat(num_samples, 1),\n",
    "        attention_mask=prompt_attention_mask.repeat(num_samples, 1),\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=max_length,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        do_sample=True,  # Must be True for temperature to take effect\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    # TODO: Missing <bos> here?\n",
    "    prompt_len = len(prompt_input_ids[0])\n",
    "    output_tokens = outputs.sequences\n",
    "    log_probs_with_grad = lm(output_tokens).logits[:, prompt_len:, :]  # [num_mc_samples, seq_len, vocab_size]\n",
    "    log_probs_with_grad = log_probs_with_grad.permute(1, 0, 2)\n",
    "\n",
    "    output_log_softmax = tuple(\n",
    "        torch.log_softmax(score_matrix, dim=-1)\n",
    "        for score_matrix in log_probs_with_grad\n",
    "    )\n",
    "    selected_log_probs = get_selected_log_probs(output_tokens, output_log_softmax, prompt_len)\n",
    "    return output_tokens, selected_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -7.5776,  -7.6723, -12.8539,  -8.5047,  -9.0768,  -7.4754,  -5.6155,\n",
       "         -7.4578,  -5.9396,  -9.2624,  -9.5459,  -7.9808,  -8.3465,  -7.3535,\n",
       "         -6.8771,  -9.5508, -10.9028, -10.4836,  -9.2769,  -9.0039,  -7.6014,\n",
       "         -8.9415,  -7.8402,  -8.8226,  -7.8247,  -7.3780,  -4.9987,  -8.5346,\n",
       "        -10.5963], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens, output_log_probs = draw_mc_samples(\"Hello\", lm, num_samples=4)\n",
    "output_log_probs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l-th output token: $y_l$\n",
    "\n",
    "Probability of the sequence: $p(y) = p(y_0) \\cdot p(y_1 | y_0) \\cdot ...$\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla \\ln p(y) &= \\nabla \\ln p(y_0) \\cdot p(y_1 | y_0) \\cdot ... \\\\\n",
    "&= \\nabla \\sum_i \\ln p(y_i \\vert y_{<i})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Baseline: -9.8361 Reward variance: 0.4873 Example sentence: But there are now many different questions that are not yet known.\n",
      "Step: 1 Baseline: -9.9320 Reward variance: 0.5705 Example sentence: What’’’\n",
      "Step: 2 Baseline: -11.8985 Reward variance: 0.7114 Example sentence: ,\"!!!!!,\"!,\"!,\"!,\"!!,\"!?\" I\n",
      "Step: 3 Baseline: -11.8068 Reward variance: 1.9457 Example sentence: you you You you you you you you you you you you you you\n",
      "Step: 4 Baseline: -11.4442 Reward variance: 1.1765 Example sentence: ,\"?\"?\"?\",\"?\"?\",\"'re,\"'d?\"?\",\",\"\n",
      "Step: 5 Baseline: -11.1384 Reward variance: 0.4877 Example sentence: iiii'di'd'd'd'd'd'd'd'd'd'd'd\n",
      "Step: 6 Baseline: -11.1759 Reward variance: 0.1344 Example sentence: iiiiiiiiiiiiiii\n",
      "Step: 7 Baseline: -12.1487 Reward variance: 1.0732 Example sentence: --\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "Step: 8 Baseline: -11.8994 Reward variance: 0.4735 Example sentence: ,\"--'m'm'm'm'm'm'm'm'm'm'm'm'm'm'm\n",
      "Step: 9 Baseline: -11.4417 Reward variance: 0.3433 Example sentence: --------,\"---'t't't\n",
      "Step: 10 Baseline: -11.1278 Reward variance: 0.3109 Example sentence: ,\",\"-------'t't't't\n",
      "Step: 11 Baseline: -12.1807 Reward variance: 0.0000 Example sentence: -------------\n",
      "Step: 12 Baseline: -12.1628 Reward variance: 0.0000 Example sentence: --------------\n",
      "Step: 13 Baseline: -12.3150 Reward variance: 0.3178 Example sentence: --------------\n",
      "Step: 14 Baseline: -12.2409 Reward variance: 0.1066 Example sentence: ----------------\n",
      "Step: 15 Baseline: -12.3013 Reward variance: 0.0295 Example sentence: ,\",\",\",\",\",\"-----------\n",
      "Step: 16 Baseline: -12.5677 Reward variance: 0.0051 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 17 Baseline: -12.5798 Reward variance: 0.1937 Example sentence: --------------,\",\",\"\n",
      "Step: 18 Baseline: -12.6263 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 19 Baseline: -12.6238 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"-\n",
      "Step: 20 Baseline: -12.5453 Reward variance: 0.3677 Example sentence: -----------------\n",
      "Step: 21 Baseline: -12.1573 Reward variance: 0.0052 Example sentence: ,\",\",\",\"---------------\n",
      "Step: 22 Baseline: -12.3552 Reward variance: 0.1356 Example sentence: -------------\n",
      "Step: 23 Baseline: -12.1438 Reward variance: 0.0011 Example sentence: ,\",\",\",\",\"---------\n",
      "Step: 24 Baseline: -12.6514 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 25 Baseline: -12.6704 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 26 Baseline: -12.6862 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 27 Baseline: -12.6696 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 28 Baseline: -12.6879 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 29 Baseline: -12.6567 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 30 Baseline: -12.6866 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 31 Baseline: -12.6799 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 32 Baseline: -12.6843 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 33 Baseline: -12.6736 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 34 Baseline: -12.6760 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 35 Baseline: -12.6795 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 36 Baseline: -12.6708 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 37 Baseline: -12.6582 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 38 Baseline: -12.6837 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 39 Baseline: -12.6824 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 40 Baseline: -12.6913 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 41 Baseline: -12.6803 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 42 Baseline: -12.6780 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 43 Baseline: -12.6853 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 44 Baseline: -12.6779 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 45 Baseline: -12.6777 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 46 Baseline: -12.6981 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 47 Baseline: -12.6886 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 48 Baseline: -12.6862 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\",\",\"\n",
      "Step: 49 Baseline: -12.6778 Reward variance: 0.0000 Example sentence: ,\",\",\",\",\",\",\",\",\",\",\",\"\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_steps = 50\n",
    "num_mc_samples = 8\n",
    "max_length_tokens = 20  # Sampling length in tokens.\n",
    "required_words = {\n",
    "    \"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\", \"pink\", \"cyan\", \"magenta\", \"turquoise\",\n",
    "    \"violin\", \"piano\", \"guitar\", \"drums\", \"flute\", \"trumpet\", \"saxophone\", \"cello\", \"clarinet\", \"harp\",\n",
    "    \"running\", \"swimming\", \"cycling\", \"painting\", \"dancing\", \"singing\", \"writing\", \"climbing\", \"skiing\", \"cooking\",\n",
    "    \"black\", \"white\", \"beige\", \"brown\", \"gray\",\n",
    "    \"banjo\", \"accordion\", \"trombone\", \"oboe\", \"mandolin\",\n",
    "    \"jogging\", \"hiking\", \"knitting\", \"gardening\", \"surfing\",\n",
    "    \"karate\", \"yoga\", \"chess\", \"fishing\", \"skating\"\n",
    "}\n",
    "\n",
    "sample_sentence = \"Hi there!\"\n",
    "\n",
    "opt = torch.optim.Adam(lm.parameters(), lr=3e-4)\n",
    "\n",
    "reward_history = []\n",
    "baseline_history = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    batch_loss = 0\n",
    "    batch_rewards = []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        # Sample prompt from text dataset.\n",
    "        prompt = random.choice(oasst1_prompts)\n",
    "        prompt_length = tokenizer(prompt, return_tensors=\"pt\")['input_ids'].shape[1]\n",
    "\n",
    "        # Sample batch of continuations for a given prompt (MC samples per prompt).\n",
    "        output_tokens, output_log_probs = draw_mc_samples(\n",
    "            prompt, lm, num_mc_samples, temperature=1.0, max_length=max_length_tokens)\n",
    "\n",
    "        # Compute sequence log probability from individual token probs.\n",
    "        sequence_log_probs = torch.stack([vec.sum() for vec in output_log_probs])\n",
    "\n",
    "        # Decode to text to compute the reward per MC sample.\n",
    "        output_texts = [tokenizer.decode(tokens) for tokens in output_tokens]  # Prompt + continuation\n",
    "        output_continuations = [tokenizer.decode(tokens) for tokens in output_tokens[:, prompt_length:]]  # Only continuation\n",
    "        rewards = torch.tensor(\n",
    "            [bert_emb_dist(text, sample_sentence) for text in output_continuations], dtype=torch.float32)\n",
    "\n",
    "        reward_baseline = rewards.mean()\n",
    "        batch_rewards.append(rewards.mean().item())\n",
    "\n",
    "        loss = -((rewards - reward_baseline) * sequence_log_probs).mean()\n",
    "        batch_loss += loss\n",
    "\n",
    "    # Take the mean loss across the batch of prompts.\n",
    "    batch_loss /= batch_size\n",
    "\n",
    "    opt.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    mean_batch_reward = sum(batch_rewards) / batch_size\n",
    "    reward_history.append(mean_batch_reward)\n",
    "    baseline_history.append(mean_batch_reward)\n",
    "\n",
    "    # Print learning progress (using first continuation from last prompt of batch)\n",
    "    sample_output = re.sub(r'\\s+', ' ', output_continuations[0]).strip()\n",
    "\n",
    "    print(\n",
    "        f\"Step: {step} \"\n",
    "        f\"Baseline: {mean_batch_reward:.4f} \"\n",
    "        f\"Reward variance: {rewards.var().item():.4f} \"\n",
    "        f\"Example sentence: {sample_output}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PDG,\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(lm, \"PDG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
