{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating Over Parameters, Tractability, MLE, MAP, Bayesian, and Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Theory Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule I**: $\\displaystyle p(a)=\\int p(a,b)\\,db$\n",
    "\n",
    "**Rule II**: $\\displaystyle p(a\\mid b)=\\frac{p(a,b)}{p(b)}$\n",
    "\n",
    "**Bayes Rule**: $\\displaystyle p(a\\mid b)=\\frac{p(b\\mid a)\\cdot p(a)}{p(b)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Method for estimating the parameter(s) of a distribution (?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given is a set of points $\\mathcal{D}=\\{y_n\\}_{n=1}^N$.\n",
    "We _choose to model_ the points with a normal distribution $\\mathcal{N}(\\mu,\\sigma)$ so we can assess the probability of a new point $y$ as $p(y;\\mu,\\sigma)\\sim\\mathcal{N}(\\mu,\\sigma)$.\n",
    "The goal is to be able to compute $p(y;\\mu,\\sigma)$ for any $y$.\n",
    "To be able to do that we need to estimate $\\mu$ and $\\sigma$ and that is what we use MLE for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is all information we have, so we want $p(\\mathcal{D};\\mu,\\sigma)$ to be high.\n",
    "We assume the data is i.i.d. given the parameters $\\mu$ and $\\sigma$.\n",
    "We can therefore write\n",
    "\n",
    "$$p(\\mathcal{D};\\mu,\\sigma)=\\prod_{n=1}^Np(y_n;\\mu,\\sigma)$$\n",
    "\n",
    "What we actually look for are the parameters.\n",
    "We are interested in the particular set of parameters for which $p(\\mathcal{D};\\mu,\\sigma)$ is maximized.\n",
    "Let $\\mu^\\star$ and $\\sigma^\\star$ denote this maximizer.\n",
    "We search for\n",
    "\\begin{align}\n",
    "\\mu^\\star=&\\arg\\max_\\mu p(\\mathcal{D};\\mu,\\sigma)\\\\\n",
    "=&\\arg\\max_\\mu\\prod_{n=1}^Np(y_n;\\mu,\\sigma)\\\\\n",
    "=&\\arg\\max_\\mu\\log\\prod_{n=1}^Np(y_n;\\mu,\\sigma)\\\\\n",
    "=&\\arg\\max_\\mu\\sum_{n=1}^N\\log p(y_n;\\mu,\\sigma)\n",
    "\\end{align}\n",
    "\n",
    "We can apply a $\\log$ to the product because it will not change the maximizer.\n",
    "\n",
    "The same formulation can be written down for $\\sigma^\\star$ to find the optimal parameter here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to _actually_ get a value for $\\mu^\\star$ we replace $p(y_n;\\mu,\\sigma)$ with the definition of our model.\n",
    "Above it is a normal distribution, so it would be TODO.\n",
    "\n",
    "Depending on the model there may be a closed-form solution for computing maximizer (i.e., the parameterization for which the data is the most likely) or one can restort to gradient descent to find it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Aposteriori (MAP)\n",
    "\n",
    "Method for estimating the parameter(s) of a distribution (?).\n",
    "Just like MLE, except the probabilistic approach is a bit different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, with MLE, we modeled $p(\\mathcal{D};\\mu,\\sigma)$, that is, the probability of the data given the parameters.\n",
    "We then looked for the parameters for which this probability is the highest.\n",
    "Now we model the probability $p(\\mu\\mid\\mathcal{D})$, that is, the probability of the parameters given the data.\n",
    "Note that $\\sigma$ is also a parameter, it's left out for readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, analogous to MLE, we seek for the argument that maximizes the probability.\n",
    "Mathematically speaking, that is\n",
    "\n",
    "$$\\mu^\\star=\\arg\\max_\\mu p(\\mu\\mid\\mathcal{D})\\,.$$\n",
    "\n",
    "Following Bayes rule, \n",
    "\n",
    "$$p(\\mu\\mid\\mathcal{D})=\\frac{p(\\mathcal{D}\\mid\\mu)\\cdot p(\\mu)}{p(\\mathcal{D})}\\,.$$\n",
    "\n",
    "We can plug the rewritten version of $p(\\mu\\mid\\mathcal{D})$ into the optimization objective from above:\n",
    "\n",
    "\\begin{align}\n",
    "\\mu^\\star=&\\arg\\max_\\mu p(\\mu\\mid\\mathcal{D})\\\\\n",
    "=&\\arg\\max_\\mu\\frac{p(\\mathcal{D}\\mid\\mu)\\cdot p(\\mu)}{p(\\mathcal{D})}\\\\\n",
    "=&\\arg\\max_\\mu p(\\mathcal{D}\\mid\\mu)\\cdot p(\\mu)\n",
    "\\end{align}\n",
    "\n",
    "Note that the denominator $p(\\mathcal{D})$ can be dropped because it does not depend on $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we did with MLE, we first expand $p(\\mathcal{D}\\mid\\mu)$ into a product over all data samples available to us, and second, apply a $\\log$ to ease the optimization, without affecting the position of the maximum (the best parameter configuration).\n",
    "\n",
    "\\begin{align}\n",
    "\\mu^\\star=&\\arg\\max_\\mu p(\\mathcal{D}\\mid\\mu)\\cdot p(\\mu)\\\\\n",
    "=&\\arg\\max_\\mu\\prod_{n=1}^Np(y_n\\mid\\mu)\\cdot p(\\mu)\\\\\n",
    "=&\\arg\\max_\\mu\\log\\left(\\prod_{n=1}^N p(y_n\\mid\\mu)\\cdot p(\\mu)\\right)\\\\\n",
    "=&\\arg\\max_\\mu\\sum_{n=1}^N\\log p(y_n\\mid\\mu)+\\log p(\\mu)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very similar to MLE, with the only difference that there is a new component in the optimization problem that we need to consider.\n",
    "It is the probability of the parameters, $p(\\mu)$, also known as the **prior**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we have to explicitly make an assumption, for example, by saying our data is normally distributed. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Modeling (?)\n",
    "\n",
    "Instead of just estimating a single value per parameter (like above just a scalar for the mean of our normal distribution), we now model the parameters as distributions themselves.\n",
    "\n",
    "What was previously $\\mu$ is now $\\mu$ but stands for a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are interested in is modeling the probability of a new (test) data point $y$ given all the information we have, namely the data $\\mathcal{D}$. It is called the **posterior predictive distribution**.\n",
    "\n",
    "$$p(y\\mid\\mathcal{D})=\\int p(y\\mid\\mu,\\mathcal{D})\\cdot p(\\mu\\mid\\mathcal{D})\\,d\\mu$$\n",
    "\n",
    "We constructed the integral using Rule I.\n",
    "The choice of $\\mu$ is an arbitrary one, one could have integrated over anything, but it is a _helpful_ one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not two components in the integral.\n",
    "The first is $p(y\\mid\\mu,\\mathcal{D})$, which is the same (?) as $p(y\\mid\\mu)$ and something we can compute.\n",
    "How? Using Bayesian inference (?) TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the second part, the so called **posterior distribution of the parameters** $p(\\mu\\mid\\mathcal{D})$, we can again apply Bayes rule as done in MAP above:\n",
    "\n",
    "$$p(\\mu\\mid\\mathcal{D})=\\frac{p(\\mathcal{D}\\mid\\mu)\\cdot p(\\mu)}{p(\\mathcal{D})}\\,.$$\n",
    "\n",
    "This time around we cannot just discard $p(\\mathcal{D})$, because we are not looking for the $\\arg\\max_\\mu$ but the actual probability $p(\\mu\\mid\\mathcal{D})$.\n",
    "We either need to find a way of computing $p(\\mathcal{D})$ or approximating it, or _know_ of some ticket (?) conjugate, ...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
